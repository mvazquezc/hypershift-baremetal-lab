= Hosted Cluster Deployment
include::_attributes.adoc[]
:profile: hypershift-baremetal-lab

The creation of the Hosted Cluster can be done by leveraging the Hosted Control Planes API resources such as `HostedCluster` and `NodePool`, in this case we will be using the Web Console. You can find the required yaml files to run this same deployment https://raw.githubusercontent.com/RHsyseng/hypershift-baremetal-lab/{branch}/lab-materials/hosted-cluster/deployment.yaml[here] (you must change the pull secret content if you want to use it).

[#creating-hosted-cluster]
== Creating a Hosted Cluster

1. Access the https://console-openshift-console.apps.management.hypershift.lab/[OpenShift Console] and login with the OpenShift admin credentials you got in the lab's email.
2. On the top bar, next to the Red Hat OpenShift logo, make sure `All Clusters` is selected. This will show us the `MultiCloud` console.
3. Once you're in, click on `Infrastructure` -> `Clusters`. You will see a screen like the one below.
+
image::mc-console.png[MultiCloud Console Overview]
+
4. Click on `Create cluster` -> `Host inventory` -> `Hosted`.
5. You will get to a wizard that will guide you through the creation of the Hosted Cluster. Make sure you enter the following details.
+
IMPORTANT: You must use OpenShift Version {hosted-cluster-version-1} even if you see a newer one.
+
.. `Infrastructure provider credential`: Leave it empty
.. `Cluster name`: hosted
.. `Cluster set`: Leave it empty
.. `Base domain`: hypershift.lab
.. `OpenShift Version`: OpenShift {hosted-cluster-version-1}
.. `Pull secret`: Put the output given by the command below:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n openshift-config \
    extract secret/pull-secret --to=- 2>/dev/null
-----
+
image::hc-wizard1.png[Hosted Cluster Wizard Screen 1]
+
6. In the next screen enter the following details.
+
.. `Controller availability policy`: Single Replica
.. `Infrastructure availability policy`: Single Replica
.. `Namespace`: hardware-inventory
.. `Use autoscaling`: Unchecked
.. `Number of hosts`: 2
+
image::hc-wizard2.png[Hosted Cluster Wizard Screen 2]
+
7. Following screen will show the networking settings, these are the settings you should use.
+
.. `API server publishing strategy`: LoadBalancer
.. `Use advanced networking`: Unchecked
.. `Show proxy settings`: Unchecked
.. `SSH public key`: Put the output given by the command below:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory \
    get infraenv/hosted -o jsonpath='{.spec.sshAuthorizedKey}'
-----
+
image::hc-wizard3.png[Hosted Cluster Wizard Screen 3]
+
8. The final screen should look like this.
+
image::hc-wizard4.png[Hosted Cluster Wizard Screen 4]
+
9. At this point you can go ahead and click `Create`.
10. After clicking you will be redirected to the `hosted` cluster view.

[#monitoring-hosted-cluster-deployment]
=== Monitoring the Hosted Cluster Deployment

In this section we will learn how we can monitor the Hosted Cluster deployment from the Web Console as well as from the CLI.

[#monitoring-hosted-cluster-deployment-webconsole]
==== Monitoring the Hosted Cluster Deployment via the Web Console

1. You should be in the `hosted` cluster view already from the previous step. If you're not you can get back to it by opening the https://console-openshift-console.apps.management.hypershift.lab/[OpenShift Console]. Making sure `All Clusters` is selected and clicking on the cluster named `hosted`.
2. You should see a screen like the one below where we see that most of the control plane components are ready.
+
IMPORTANT: It can take up to 5 minutes for the conditions to look like in the screenshot below.
+
image::hosted-cluster-view1.png[Hosted Cluster View 1]
+
3. In this same view, we can click on the `NodePool` to see the nodes deployment status. We can see at this point two bare metal hosts from the inventory (hosted-worker0 and hosted-worker2) being installed.
+
IMPORTANT: The bare metal hosts selected may be different for your cluster.
+
image::hosted-cluster-view2.png[Hosted Cluster View 2]
+
4. After a few moments (around 10 minutes), we will see that the nodes have been installed.
+
image::hosted-cluster-view3.png[Hosted Cluster View 3]
+
5. Additionally, we can click on `Nodes` in the hosted cluster view and we will see that the two nodes joined the Hosted Cluster.
+
image::hosted-cluster-view4.png[Hosted Cluster View 4]
+
6. At this point the Hosted Cluster is almost installed. In the next section we will see how to monitor the deployment from the CLI.

[#monitoring-hosted-cluster-deployment-cli]
==== Monitoring the Hosted Cluster Deployment via the CLI

IMPORTANT: Below commands must be executed from the workstation host if not specified otherwise.

1. Check the `HostedCluster` for the hosted cluster, you can see it says the hosted control plane is ready but the progress is `Partial`. This is expected since the Hosted Cluster deployment is not finished yet.
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    hostedcluster hosted
-----
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME     VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
hosted             hosted-admin-kubeconfig   Partial    True        False         The hosted control plane is available
-----
+
2. We can check the control plane pods.
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted-hosted get pods
-----
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                                  READY   STATUS    RESTARTS      AGE
capi-provider-5f67665fc-s92cf                         1/1     Running   0             21m
catalog-operator-7b8589ccff-wvsff                     2/2     Running   2 (18m ago)   18m
certified-operators-catalog-96c5fd665-ctkx2           1/1     Running   0             18m
cluster-api-6949888596-7v2fg                          1/1     Running   0             21m
cluster-autoscaler-57fd57cd5-mqwdx                    1/1     Running   0             18m
cluster-image-registry-operator-866c55fb67-klr56      2/2     Running   0             18m
cluster-network-operator-85d5b5c9cf-kf8ht             2/2     Running   0             18m
cluster-node-tuning-operator-cc669ffc4-9cq85          1/1     Running   0             18m
cluster-policy-controller-7479fb5685-tvwdz            1/1     Running   0             18m
cluster-storage-operator-59fb6b99fb-pn95w             1/1     Running   0             18m
cluster-version-operator-7b747f4d8d-x9fp4             1/1     Running   0             18m
community-operators-catalog-74b7c8d66d-qg2fg          1/1     Running   0             18m
control-plane-operator-7489787949-zk4jk               1/1     Running   0             21m
control-plane-pki-operator-b545ddf55-dtj9w            1/1     Running   0             21m
csi-snapshot-controller-5886f8d4bc-5dbr6              1/1     Running   0             18m
csi-snapshot-controller-operator-d8b554675-fdl8d      1/1     Running   0             18m
csi-snapshot-webhook-7cb9d884-9s5ms                   1/1     Running   0             18m
dns-operator-58866f6b5-78d86                          1/1     Running   0             18m
etcd-0                                                3/3     Running   0             21m
hosted-cluster-config-operator-9f8db7c98-zlv8h        1/1     Running   0             18m
ignition-server-7f58bcdcf8-4mr4c                      1/1     Running   0             18m
ignition-server-proxy-7494b7d785-7zvvg                1/1     Running   0             18m
ingress-operator-d5f4c6d6-nhqb9                       2/2     Running   0             18m
konnectivity-agent-6c5485d9bb-vwln7                   1/1     Running   0             18m
kube-apiserver-7c5bb9c695-vtt2d                       4/4     Running   0             20m
kube-controller-manager-544dd8d4f9-k2fr7              1/1     Running   0             11m
kube-scheduler-56477d87bd-qrfq2                       1/1     Running   0             19m
machine-approver-7b85988895-7cvc6                     1/1     Running   0             18m
multus-admission-controller-74ccdf8dc4-486f2          2/2     Running   0             12m
network-node-identity-546c9c477-rtqcd                 3/3     Running   0             12m
oauth-openshift-69dd979f7c-nh96v                      2/2     Running   0             18m
olm-operator-774bf68cb4-7w9k7                         2/2     Running   0             18m
openshift-apiserver-6b848587cf-dxvrv                  3/3     Running   0             11m
openshift-controller-manager-74c45dbcb7-p2gr7         1/1     Running   0             18m
openshift-oauth-apiserver-758dfd9bb4-qr565            2/2     Running   0             18m
openshift-route-controller-manager-576d8b7cfd-rldnv   1/1     Running   0             18m
ovnkube-control-plane-bfc6bb4fd-tl8rd                 3/3     Running   0             12m
packageserver-7f97b955c6-slctl                        2/2     Running   0             18m
redhat-marketplace-catalog-5fcdbb6578-gsljc           1/1     Running   0             18m
redhat-operators-catalog-5b5b659b64-8cknx             1/1     Running   0             18m
-----
+
3. The NodePool will tell us the state of the nodes joining the Hosted Cluster:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    nodepool nodepool-hosted-1
-----
+
IMPORTANT: At this point the nodes are still being deployed as you can see.
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
nodepool-hosted-1   hosted    2               0               False         False        {hosted-cluster-version-1}                                       Scaling up MachineSet to 2 replicas (actual 0)
-----
+
4. Since the nodes are being deployed we can check the status of the agents:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory get agents
-----
+
IMPORTANT: We can see at this point two agents from the inventory being rebooted as part of the installation process.
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                   CLUSTER   APPROVED   ROLE          STAGE
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201             true       auto-assign   
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0202   hosted    true       worker        Rebooting
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0203   hosted    true       worker        Rebooting
-----
+
5. After a few moments (up to 10 minutes), the agents will be fully installed:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory get agents
-----
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                   CLUSTER   APPROVED   ROLE          STAGE
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201             true       auto-assign   
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0202   hosted    true       worker        Done
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0203   hosted    true       worker        Done
-----
+
6. And the NodePool will reflect that as well.
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    nodepool nodepool-hosted-1
-----
+
[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
nodepool-hosted-1   hosted    2               2               False         False        {hosted-cluster-version-1}                                      
-----

At this point the Hosted Cluster deployment is not finished yet, since we need to fix Ingress for the cluster to be fully deployed. We will do that in the next section where we will learn how to access the Hosted Cluster.
